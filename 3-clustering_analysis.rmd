---
title: "Cluster Analysis"
author: "Victoria Costas"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
---

```{r global_options, include=T, echo = F}
knitr::opts_chunk$set(echo = T, warning=FALSE, message=FALSE)
```

```{=html}
<style>
body {
text-align: justify}
</style>
```

## Libraries

```{r}
library(tidyverse)
library(dplyr)
library(mice)
library(zoo)
library(randomForest)
library(missForest)
library(ggplot2)
library(patchwork)
library(factoextra)
library(plotly)
```

## Read the data

```{r}
data <- read_csv("./data/merged_data.csv")

#remove the first row for clarity
data <- data[, -1]
```

### Descriptive analysis

Some preliminary analysis:

```{r}
summary(data)
```

In this first look at the database we can see that there are quite a few missing values and therefore in the following sections we will see how to deal with them.

### Preprocessing

```{r}
# Remove columns from 2000 to 2009 and column 2022
columns_to_remove <- c(2000:2009, 2022)
data <- data[, !names(data) %in% columns_to_remove]

#order the columns
data <- data[, rev(order(colnames(data)))]

#Check NAs
na_percentage_by_country <- data.frame(geo_time = data$geo_time,
                                       na_percentage = apply(data[, -(1:2)], 1, function(x) sum(is.na(x)) / length(x) * 100))

na_percentage_by_year <- data.frame(year = colnames(data)[-(1:2)],
                                    na_percentage = apply(data[, -(1:2)], 2, function(x) sum(is.na(x)) / length(x) * 100))

```

As a first step we will remove the columns with the highest number of missing values as they are a hindrance to the analysis and actually imputing so many missing values greatly alters the results. In order to proceed with data imputation we first have to modify the database to a format suitable for cluster analysis.

```{r}
# Reshape the dataset to long format
data_long <- data %>%
  pivot_longer(cols = starts_with("20"), names_to = "year", values_to = "value")

# Remove duplicates and compute the mean by indicator, country, and year
data_mean <- data_long %>%
  group_by(indicator, geo_time, year) %>%
  summarize(mean_value = mean(value, na.rm = TRUE))

# Pivot the dataset to wide format with indicator values as column names
data_wide <- data_mean %>%
  pivot_wider(names_from = indicator, values_from = mean_value)

```

### Imputation

In the following, we will apply three different imputation methods, the first one based on linear regression and the following ones are the methods of random forest and miss forest. 

```{r}
# Remove the 'X' prefix from the 'year' column
data_wide$year <- sub("^X", "", data_wide$year)
#Change NaN to NA
data_wide <- lapply(data_wide, function(x) {x[is.nan(x)] <- NA; x})
data_wide <- do.call(data.frame, data_wide)

data_wide <- subset(data_wide, !geo_time %in% c("KR", "US", "JP", "XK"))

#Linear regression
mice_object <- mice(data_wide[, -(1:2)], method = "norm", m = 10)
imputed_data <- complete(mice_object)
data_lm <- cbind(data_wide[, 1:2], imputed_data)

#Random Forest
mice_object <- mice(data_wide[, -(1:2)], method = "rf")
imputed_data <- complete(mice_object)
data_rf <- cbind(data_wide[, 1:2], imputed_data)

#Miss Forest
imputed_data <- missForest(data_wide[, -(1:2)])
data_mf <- cbind(data_wide[, 1:2], imputed_data$ximp)
```

Finally, to continue with all the analyses we want to do, we will use the database obtained from the miss forest imputation as it is the one that most closely resembles the original distribution of values. Moreover, since this database will be used repeatedly, it will be stored locally.

```{r}
#save the dataset for other analysis
write.csv(data_mf, file = "./data/data_mf.csv")
```

## Cluster analysis

### With the complete temporal series

This analysis can only be done with numeric variables, so non numeric variables will be excluded from the dataset used for clustering.

```{r}
X <- data_mf %>% select(-c(year, geo_time))
```

We will set 5 as the starting number of clusters, as it seems reasonable for the number of observations and countries, later we will check with the "Elbow" method if another number of clusters is more recommendable.

```{r}
set.seed(123) #set seed for reproducibility
k = 5
fit <- kmeans(scale(X), centers = k, nstart = 10000)
groups <- fit$cluster
barplot(table(groups), col = "#FF00FF")
cluster_labels <- fit$cluster
```

If we look at the bar chart we can see that there are two very similar clusters (3 and 5) which may mean that another number of clusters may fit the data better, which we will check below. In addition, there is one very large cluster and one quite insignificant cluster. In this case 50.8% of the total sum of squares is explained by the variability between the clusters. 

```{r}
fviz_nbclust(scale(X), kmeans, method = 'wss', k.max = 10, nstart = 1000) 
```

With regard to the ideal number of clusters, according to the previous graph it would be around 6, so the number of clusters we started with was not unreasonable. The analysis will be repeated with 7 clusters instead of 5. 

```{r}
set.seed(123)
k = 7
fit <- kmeans(scale(X), centers = k, nstart = 10000)
groups <- fit$cluster
barplot(table(groups), col = "#FF6666")
cluster_labels <- fit$cluster
```

In this case, we can see that there is quite a difference between all the clusters, with one cluster standing out above the rest and being much more numerous than the rest.

```{r}
centers=fit$centers

barplot(centers[1,], las=2, col="#FF6666")
barplot(centers[2,], las=2, col="#FF6666")
barplot(centers[3,], las=2, col="#FF6666")
barplot(centers[4,], las=2, col="#FF6666")
barplot(centers[5,], las=2, col="#FF6666")
barplot(centers[6,], las=2, col="#FF6666")
barplot(centers[7,], las=2, col="#FF6666")
```

In the above bar charts we can see the characteristics of the clusters. There are two quite similar clusters in which their goals are close to zero except for a few exceptions such as goal 13 or 2 of the second cluster or goal 10 of the sixth cluster. Cluster number seven has mainly negative values, while cluster number three is the opposite, with the score of the goals being almost always positive except for some goals which are close to 0. 

## Clusplot 

In the following graph we can see how these clusters are distributed and which countries belong to which cluster.

```{r}
names <- data_mf$geo_time

#clusplot
p <- fviz_cluster(fit, data = X, geom = c("point"), ellipse.type = 'norm', pointsize=1)+
  theme_minimal()+ geom_text(label=names,hjust=0, vjust=0,size=2,check_overlap = T) + scale_fill_brewer(palette="Paired")
p
```


```{r}
ggplotly(p, tooltip = "text")
```

It seems that in some goals they belong to the same cluster but in the majority they donÂ´t.

```{r}
fit.km <-eclust(X, "kmeans", stand = T, k = 7, graph = T)
```

In this last graph we observe the boundaries of each cluster.

```{r}
#Compute the average ind value by cluster
data_mf$cluster <- cluster_labels
avg_indicators <- aggregate(. ~ cluster, data = subset(data_mf, select = -c(geo_time, year)), FUN = mean)
```

### Years 2010-2015

In order to perform comparative tasks we will split the dataset into two different time series from 2010 to 2015 and the other from 2016 to 2021. 

```{r}
data_old <- subset(data_mf, year %in% c(2010, 2011, 2012, 2013, 2014, 2015))
```

This analysis can only be done with numeric variables, so non numeric variables will be excluded from the dataset used for clustering.

```{r}
X <- data_old %>% select(-c(year, geo_time))
```

We will set 5 as the starting number of clusters, as it seems reasonable for the number of observations and countries, later we will check with the "Elbow" method if another number of clusters is more recommendable.

```{r}
set.seed(123) #set seed for reproducibility
k = 5
fit <- kmeans(scale(X), centers = k, nstart = 10000)
groups <- fit$cluster
barplot(table(groups), col = "#FF00FF")
cluster_labels <- fit$cluster
```

If we look at the bar chart we can see that there are two very similar clusters (1 and 5) which may mean that another number of clusters may fit the data better, which we will check below. In addition, there is one very large cluster and one quite insignificant cluster. In this case 56.7% of the total sum of squares is explained by the variability between the clusters. 

```{r}
fviz_nbclust(scale(X), kmeans, method = 'wss', k.max = 10, nstart = 1000) 
```

With regard to the ideal number of clusters, according to the previous graph it would be around 6, so the number of clusters we started with was not unreasonable. The analysis will be repeated with 7 clusters instead of 5. 

```{r}
set.seed(123)
k = 7
fit <- kmeans(scale(X), centers = k, nstart = 10000)
groups <- fit$cluster
barplot(table(groups), col = "#FF6666")
cluster_labels <- fit$cluster
```

In this case, we can see that there is quite a difference between all the clusters, with two clusters standing out above the rest and being much more numerous than the rest.

```{r}
centers=fit$centers

barplot(centers[1,], las=2, col="#FF6666")
barplot(centers[2,], las=2, col="#FF6666")
barplot(centers[3,], las=2, col="#FF6666")
barplot(centers[4,], las=2, col="#FF6666")
barplot(centers[5,], las=2, col="#FF6666")
barplot(centers[6,], las=2, col="#FF6666")
barplot(centers[7,], las=2, col="#FF6666")
```

In this case, the goals' values distribute more or less as in the previous bar charts (for the whole time series).

## Clusplot 

In the following graph we can see how these clusters are distributed and which countries belong to which cluster.

```{r}
names <- data_old$geo_time

#clusplot
p <- fviz_cluster(fit, data = X, geom = c("point"), ellipse.type = 'norm', pointsize=1)+
  theme_minimal()+ geom_text(label=names,hjust=0, vjust=0,size=2,check_overlap = T) + scale_fill_brewer(palette="Paired")
p
```


```{r}
ggplotly(p, tooltip = "text")
```

We can clearly observe that Spain and Germany don't belong to the same cluster, which may be due to the fact that they have different characteristics as a country.

```{r}
fit.km <-eclust(X, "kmeans", stand = T, k = 7, graph = T)
```

```{r}
#Compute the average ind value by cluster
data_old$cluster <- cluster_labels
avg_indicators <- aggregate(. ~ cluster, data = subset(data_old, select = -c(geo_time, year)), FUN = mean)
```


### Years 2016-2021

```{r}
data_new <- subset(data_mf, year %in% c(2016, 2017, 2018, 2019, 2020, 2021))
```

This analysis can only be done with numeric variables, so non numeric variables will be excluded from the dataset used for clustering.

```{r}
X <- data_new %>% select(-c(year, geo_time))
```

We will set 5 as the starting number of clusters, as in the previous ones, as it seems reasonable for the number of observations and countries, later we will check with the "Elbow" method if another number of clusters is more recommendable.

```{r}
set.seed(123) #set seed for reproducibility
k = 5
fit <- kmeans(scale(X), centers = k, nstart = 10000)
groups <- fit$cluster
barplot(table(groups), col = "#FF00FF")
cluster_labels <- fit$cluster
```

It seems that another number of clusters is more reasonable:

```{r}
fviz_nbclust(scale(X), kmeans, method = 'wss', k.max = 10, nstart = 1000) 
```

With regard to the ideal number of clusters, according to the previous graph it would be around 6. The analysis will be repeated with 6 clusters instead of 5. 

```{r}
set.seed(123)
k = 6
fit <- kmeans(scale(X), centers = k, nstart = 10000)
groups <- fit$cluster
barplot(table(groups), col = "#FF6666")
cluster_labels <- fit$cluster
```

In this case, there are two clusters much more numerous than the rest. 

```{r}
centers=fit$centers

barplot(centers[1,], las=2, col="#FF6666")
barplot(centers[2,], las=2, col="#FF6666")
barplot(centers[3,], las=2, col="#FF6666")
barplot(centers[4,], las=2, col="#FF6666")
barplot(centers[5,], las=2, col="#FF6666")
barplot(centers[6,], las=2, col="#FF6666")
```

The charts are very similar to the previous ones, but there are more positive values (and they are higher).

## Clusplot 

In the following graph we can see how these clusters are distributed and which countries belong to which cluster.

```{r}
names <- data_new$geo_time

#clusplot
p <- fviz_cluster(fit, data = X, geom = c("point"), ellipse.type = 'norm', pointsize=1)+
  theme_minimal()+ geom_text(label=names,hjust=0, vjust=0,size=2,check_overlap = T) + scale_fill_brewer(palette="Paired")
p
```


```{r}
ggplotly(p, tooltip = "text")
```

We can clearly observe that Spain and Germany belong to the same cluster, which may be due to the fact that they have adopted similar strategies to achieve the objectives of the SDGs.

```{r}
fit.km <-eclust(X, "kmeans", stand = T, k = 4, graph = T)
```


```{r}
#Compute the average ind value by cluster
data_new$cluster <- cluster_labels
avg_indicators <- aggregate(. ~ cluster, data = subset(data_new, select = -c(geo_time, year)), FUN = mean)
```


```
#### For the document table of averages
# Filter the data for the years 2010 and 2020
filtered_data_2010 <- data_mf %>%
  filter(year == 2010)

filtered_data_2020 <- data_mf %>%
  filter(year == 2020)

country_avg_2010 <- filtered_data_2010 %>%
  group_by(geo_time)
rowMeans(country_avg_2010[-1:-2])

country_avg_2020 <- filtered_data_2020 %>%
  group_by(geo_time)
rowMeans(country_avg_2020[-1:-2])
```
